# ðŸ” NLP RNN Sentiment Analysis â€“ IMDB Movie Reviews

## ðŸŽ¯ Business Objective
The goal of this project is to classify IMDB movie reviews as **Positive** or **Negative** using Recurrent Neural Network (RNN) variants. It simulates a real-world scenario where businesses, content platforms, or review aggregators need to analyze customer sentiment at scale to improve user experience, identify trends, or moderate content.

---

## ðŸ“š Dataset
- **Source**: IMDB movie review dataset (Keras built-in)
- **Type**: Binary Sentiment Classification
- **Instances**: 50,000 (25,000 training, 25,000 test)
- **Classes**: `0` - Negative, `1` - Positive
- **Preprocessed**: Encoded as sequences of integers

---

## âš™ï¸ Methodology
1. **Data Preprocessing**
   - Load IMDB dataset using Keras
   - Limit vocabulary size to top 10,000 frequent words
   - Pad sequences to a uniform length (500 tokens)

2. **Model Development**
   - Implemented three deep learning architectures:
     - Simple RNN
     - LSTM (Long Short-Term Memory)
     - GRU (Gated Recurrent Unit)
   - Models trained using binary cross-entropy loss and Adam optimizer

3. **Evaluation**
   - Performance compared using:
     - Accuracy
     - Precision
     - Recall
     - F1-Score
     - Confusion Matrix
   - Visualized via Jupyter Notebook dashboard

4. **Deployment**
   - Interactive Streamlit app allows users to input text and view predictions
   - Dockerfile for containerization
   - GitHub Actions workflow for CI/CD integration

---

## ðŸ§  Models Used
| Model Type | Architecture | Key Layer Size |
|------------|--------------|----------------|
| RNN        | Embedding + SimpleRNN + Dense | 32 units |
| LSTM       | Embedding + LSTM + Dense      | 64 units |
| GRU        | Embedding + GRU + Dense       | 64 units |

---

## ðŸ’» How to Run This Project

### ðŸ”§ Step 1: Install Requirements
```bash
pip install -r requirements.txt
```

### ðŸ‹ï¸ Step 2: Train Models
```bash
python scripts/train_rnn.py
python scripts/train_lstm.py
python scripts/train_gru.py
```

### ðŸ“Š Step 3: Evaluate Models
Open the notebook:
```
notebooks/comparison_dashboard.ipynb
```

### ðŸŒ Step 4: Launch Streamlit App
```bash
streamlit run streamlit_app/app.py
```

### ðŸ³ Step 5: Run with Docker
```bash
docker build -t sentiment-app .
docker run -p 8501:8501 sentiment-app
```

---

## ðŸ“¦ Folder Structure
```
nlp_rnn_sentiment_analysis/
â”‚
â”œâ”€â”€ data/                    # Placeholder for raw or future data
â”œâ”€â”€ models/                  # Trained model weights (RNN, LSTM, GRU)
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ comparison_dashboard.ipynb
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_rnn.py
â”‚   â”œâ”€â”€ train_lstm.py
â”‚   â””â”€â”€ train_gru.py
â”œâ”€â”€ streamlit_app/
â”‚   â””â”€â”€ app.py
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ data_utils.py
â”œâ”€â”€ .github/workflows/       # GitHub Actions CI setup
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â””â”€â”€ README.md
```

---

## ðŸ“ˆ Output
- Classification reports printed for all 3 models
- Confusion matrices for visual analysis
- Sentiment predictions live via Streamlit

---

## ðŸ§ª Future Enhancements
- Integrate Hugging Face transformers (e.g., BERT)
- Add explainability using SHAP or LIME
- Visualize attention in LSTM/GRU
- Streamlit enhancements: prediction confidence bar, batch input

---

## ðŸ·ï¸ Tags
`NLP` `RNN` `LSTM` `GRU` `Sentiment Analysis` `Streamlit` `IMDB` `Docker` `AI Deployment` `GitHub Actions`

---

## ðŸš€ Advanced Models & Enhancements

### ðŸ” Bidirectional RNNs
- **Bidirectional LSTM**: Captures both forward and backward context for improved sentiment detection.
- **Bidirectional GRU**: Lighter alternative to LSTM with similar performance.
- Both models trained with frozen **GloVe 100D word embeddings**.

### ðŸ§  Pre-trained Word Embeddings
- Integrated **GloVe (Global Vectors for Word Representation)**.
- Helps model understand semantic similarity (e.g., â€œawesomeâ€ and â€œgreatâ€ have similar vectors).

### ðŸ§ª Hyperparameter Tuning
- Implemented **KerasTuner Hyperband** search for optimal model configuration:
  - Tuned embedding output dimension
  - Tuned number of LSTM units

### ðŸ†• Additional Scripts
| File | Purpose |
|------|---------|
| `train_bidirectional_lstm_glove.py` | Train BiLSTM with GloVe |
| `train_bidirectional_gru_glove.py`  | Train BiGRU with GloVe  |
| `tune_lstm_kerastuner.py`          | Auto-tune LSTM using KerasTuner |

---

## ðŸ“ˆ Evaluation Metrics
Each model is evaluated using:
- Accuracy
- Precision
- Recall
- F1-Score
- Confusion Matrix (via dashboard notebook)

> Use the Jupyter notebook `comparison_dashboard.ipynb` to compare results of basic and advanced models visually.
