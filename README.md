
---

# ğŸ” NLP RNN Sentiment Analysis â€“ IMDB Movie Reviews

## ğŸ¯ Business Objective

This project classifies IMDB movie reviews as **Positive** or **Negative** using various Recurrent Neural Network (RNN) architectures. It simulates real-world sentiment analysis for platforms aiming to monitor user opinion at scaleâ€”enhancing recommendation systems, moderation, and marketing analytics.

---

## ğŸ“š Dataset

* **Source**: IMDB Movie Review Dataset (Keras built-in)
* **Task**: Binary Sentiment Classification
* **Size**: 50,000 reviews (25k train / 25k test)
* **Classes**: `0` - Negative, `1` - Positive
* **Format**: Preprocessed and integer-encoded word sequences

---

## âš™ï¸ Methodology

1. **Data Preprocessing**

   * Load the IMDB dataset with top 10,000 frequent words.
   * Pad sequences to a fixed length of 500 tokens.

2. **Model Development**

   * Implemented the following deep learning models:

     * Simple RNN
     * LSTM (Long Short-Term Memory)
     * GRU (Gated Recurrent Unit)
     * Bidirectional LSTM (GloVe)
     * Bidirectional GRU (GloVe)
     * Tuned LSTM using KerasTuner
   * Trained using **binary cross-entropy** and the **Adam optimizer**

3. **Evaluation**

   * Used metrics: Accuracy, Precision, Recall, F1-score, Confusion Matrix
   * Results visualized in Jupyter dashboard

4. **Deployment**

   * **Streamlit** app for real-time prediction
   * **Docker** for containerized deployment
   * **GitHub Actions** for CI/CD automation

---

## ğŸ§  Models Used

| Model Type             | Architecture                   | Key Layers/Parameters   |
| ---------------------- | ------------------------------ | ----------------------- |
| Simple RNN             | Embedding + SimpleRNN + Dense  | 32 RNN units            |
| LSTM                   | Embedding + LSTM + Dense       | 64 LSTM units           |
| GRU                    | Embedding + GRU + Dense        | 64 GRU units            |
| BiLSTM (GloVe)         | GloVe + BiLSTM + Dense         | 64 BiLSTM units, frozen |
| BiGRU (GloVe)          | GloVe + BiGRU + Dense          | 64 BiGRU units, frozen  |
| Best LSTM (KerasTuner) | Tuned Embedding + LSTM + Dense | Best via Hyperband      |

---

## ğŸ’» How to Run This Project

### ğŸ”§ Step 1: Install Dependencies

```bash
pip install -r requirements.txt
```

### ğŸ—‚ï¸ Step 2: Download GloVe Embeddings

Download GloVe (100D) from:
[http://nlp.stanford.edu/data/glove.6B.zip](http://nlp.stanford.edu/data/glove.6B.zip)

Extract and place `glove.6B.100d.txt` in the `data/` directory.

---

### ğŸ‹ï¸ Step 3: Train Models

```bash
python scripts/train_rnn.py
python scripts/train_lstm.py
python scripts/train_gru.py
python scripts/train_bidirectional_lstm_glove.py
python scripts/train_bidirectional_gru_glove.py
python scripts/tune_lstm_kerastuner.py
```

---

### ğŸ“Š Step 4: Evaluate Models

Open Jupyter Notebook:

```bash
notebooks/comparison_dashboard.ipynb
```

---

### ğŸŒ Step 5: Launch Streamlit App

```bash
streamlit run streamlit_app/app.py
```

---

### ğŸ³ Step 6: Run with Docker

```bash
docker build -t sentiment-app .
docker run -p 8501:8501 sentiment-app
```

---

## ğŸ“¦ Folder Structure

```
nlp_rnn_sentiment_analysis/
â”‚
â”œâ”€â”€ data/                      # GloVe file goes here
â”œâ”€â”€ models/                    # Trained model weights
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ comparison_dashboard.ipynb
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_rnn.py
â”‚   â”œâ”€â”€ train_lstm.py
â”‚   â”œâ”€â”€ train_gru.py
â”‚   â”œâ”€â”€ train_bidirectional_lstm_glove.py
â”‚   â”œâ”€â”€ train_bidirectional_gru_glove.py
â”‚   â””â”€â”€ tune_lstm_kerastuner.py
â”œâ”€â”€ streamlit_app/
â”‚   â””â”€â”€ app.py
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ data_utils.py
â”œâ”€â”€ .github/workflows/         # CI/CD pipeline
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â””â”€â”€ README.md
```

---

## ğŸ“ˆ Evaluation Output

* Printed classification reports
* Confusion matrix for each model
* Live predictions through the Streamlit interface

---

## ğŸ”® Future Enhancements

* Integrate BERT using Hugging Face Transformers
* Explainability tools like **LIME** or **SHAP**
* Visualize LSTM/GRU attention weights
* Enhance Streamlit with:

  * Confidence visualization
  * Batch prediction input

---

## ğŸš€ Advanced Models & Techniques

### ğŸ” Bidirectional RNNs

* **BiLSTM & BiGRU** capture past and future context.
* Powered by **pre-trained GloVe (100D)** word vectors.
* Embeddings are frozen for stable semantics.

### ğŸ§  Word Embeddings (GloVe)

* GloVe vectors help understand word similarity (e.g., "good" â‰ˆ "excellent")
* Source: Stanford NLP ([Download link](http://nlp.stanford.edu/data/glove.6B.zip))

### ğŸ¯ Hyperparameter Tuning

* Used **KerasTuner Hyperband** to optimize:

  * Embedding output dimension
  * Number of LSTM units

---

## ğŸ“ˆ Evaluation Metrics Used

Each model was evaluated using:

* Accuracy
* Precision
* Recall
* F1-Score
* Confusion Matrix

> ğŸ“Š Compare results in: `notebooks/comparison_dashboard.ipynb`

---

## ğŸ“ Repository

Clone this project:

```bash
git clone https://github.com/amitkharche/NLP_RNN_LSTM_GRU_sentiment_analysis
cd NLP_RNN_LSTM_GRU_sentiment_analysis
```

---

## ğŸ·ï¸ Tags

`NLP` `Sentiment Analysis` `IMDB` `RNN` `LSTM` `GRU`
`Bidirectional` `GloVe` `KerasTuner` `Streamlit` `Docker`
`Deep Learning` `Text Classification` `AI Deployment`

---

## Contact

If you have questions or want to collaborate, feel free to connect with me on
- [LinkedIn](https://www.linkedin.com/in/amit-kharche)  
- [Medium](https://medium.com/@amitkharche14)  
- [GitHub](https://github.com/amitkharche)